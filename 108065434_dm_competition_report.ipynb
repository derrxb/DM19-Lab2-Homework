{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Information\n",
    "Name: Derrick Bol\n",
    "\n",
    "Student ID: 108065434\n",
    "\n",
    "GitHub ID: @derrxb\n",
    "\n",
    "Kaggle name: @derrxb\n",
    "\n",
    "Kaggle private scoreboard snapshot:\n",
    "\n",
    "[Snapshot](img/pic0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First: __This part is worth 30% of your grade.__ Do the **take home** exercises in the [DM19-Lab2-Master Repo](https://github.com/EvaArevalo/DM19-Lab2-Master). You may need to copy some cells from the Lab notebook to this notebook. \n",
    "\n",
    "\n",
    "2. Second: __This part is worth 30% of your grade.__ Participate in the in-class [Kaggle Competition](https://www.kaggle.com/t/179d01d4dd984fc5ac45a894822479dd) regarding Emotion Recognition on Twitter. The scoring will be given according to your place in the Private Leaderboard ranking: \n",
    "    - **Bottom 40%**: Get 20% of the score (ie. 20% of 30% )\n",
    "\n",
    "    - **Top 41% - 100%**: Get (101-x)% of the score, where x is your ranking in the leaderboard (ie. (101-x)% of 30% )   \n",
    "    Submit your last submission __BEFORE the deadline (Nov. 23rd 11:59 pm, Saturday)__. Make sure to take a screenshot of your position at the end of the competition and store it as '''pic0.png''' under the **img** folder of this repository and rerun the cell **Student Information**.\n",
    "    \n",
    "\n",
    "3. Third: __This part is worth 30% of your grade.__ A report of your work developping the model for the competition (You can use code and comment it). This report should include what your preprocessing steps, the feature engineering steps and an explanation of your model. You can also mention different things you tried and insights you gained. \n",
    "\n",
    "\n",
    "4. Fourth: __This part is worth 10% of your grade.__ It's hard for us to follow if your code is messy :'(, so please **tidy up your notebook** and **add minimal comments where needed**.\n",
    "\n",
    "\n",
    "You can submit your homework following these guidelines: [Git Intro & How to hand your homework](https://github.com/EvaArevalo/DM19-Lab1-Master/blob/master/Git%20Intro%20%26%20How%20to%20hand%20your%20homework.ipynb), but make sure to fork the [DM19-Lab2-Homework](https://github.com/EvaArevalo/DM19-Lab2-Homework) repository this time! Also please __DON´T UPLOAD HUGE DOCUMENTS__, please use Git ignore for that.\n",
    "\n",
    "Make sure to commit and save your changes to your repository __BEFORE the deadline (Nov. 26th 11:59 pm, Tuesday)__. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Kaggle Competition Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of contents\n",
    "* Summary\n",
    "* Data Preparation\n",
    "* Data Preprocessing\n",
    "* Feature engineering\n",
    "* Models\n",
    "* Final Model Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "This report is an overview of the various steps completed for the Data Mining class' Twitter sentiment analysis competition. It includes the various steps such as data preparation, preprocessing, and the various models used to build my model for the competition.\n",
    "\n",
    "Overall, 4 major models were tried. These include Random Forest Tree, Naive Bayes classifier, Logistic Regression, and a neural network. The best result was obtained with the naive bayes model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset was provided in different dataset and so a major part of the data preparation was merging the different files into one Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from nltk.corpus import stopwords\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>identification</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tweet_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0x28cc61</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0x29e452</td>\n",
       "      <td>train</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0x2b3819</td>\n",
       "      <td>train</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0x2db41f</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0x2a2acc</td>\n",
       "      <td>train</td>\n",
       "      <td>trust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0x227e25</td>\n",
       "      <td>train</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0x293813</td>\n",
       "      <td>train</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0x1e1a7e</td>\n",
       "      <td>train</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0x2156a5</td>\n",
       "      <td>train</td>\n",
       "      <td>trust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0x2bb9d2</td>\n",
       "      <td>train</td>\n",
       "      <td>trust</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1867535 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         identification  emotion\n",
       "tweet_id                        \n",
       "0x28cc61           test      NaN\n",
       "0x29e452          train      joy\n",
       "0x2b3819          train      joy\n",
       "0x2db41f           test      NaN\n",
       "0x2a2acc          train    trust\n",
       "...                 ...      ...\n",
       "0x227e25          train  disgust\n",
       "0x293813          train  sadness\n",
       "0x1e1a7e          train      joy\n",
       "0x2156a5          train    trust\n",
       "0x2bb9d2          train    trust\n",
       "\n",
       "[1867535 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion = pd.read_csv('emotion.csv')\n",
    "emotion.set_index('tweet_id')\n",
    "\n",
    "identification = pd.read_csv('data_identification.csv')\n",
    "identification.set_index('tweet_id')\n",
    "\n",
    "identification_with_emotion = pd.merge(identification, emotion, on=\"tweet_id\", how=\"outer\", left_index=True)\n",
    "identification_with_emotion.set_index('tweet_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The `tweets_DM.json` is a json file with multiple lines of JSON objects. We need to parse each line individualy.\n",
    "# We then normalize the results so we can merge it with the `identification_with_emotion` dataframe.\n",
    "raw_tweets = []\n",
    "\n",
    "with open('tweets_DM.json') as f:\n",
    "    for line in f:\n",
    "        raw_tweets.append(json.loads(line))\n",
    "        \n",
    "temp_tweets = pd.io.json.json_normalize(raw_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns and drop uncessary columns.\n",
    "temp_tweets.columns = ['score', 'index', 'crawldate', 'type', 'hastags', 'tweet_id', 'text']\n",
    "temp_tweets.set_index('tweet_id')\n",
    "temp_tweets = temp_tweets.drop(['index', 'type'], 1)\n",
    "\n",
    "# Merge temp_tweets with the emotion and training information\n",
    "tweets = pd.merge(temp_tweets, identification_with_emotion, on=\"tweet_id\", left_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the data preprocessing step, the main thing I did to the dataset was to clean the tweets. I cleaned the tweets by removing stopwords, removing user mentions, and removing unnecessary punctuations. This can be seen in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning method uses regex to find and replace text.\n",
    "def clean_data(tt):\n",
    "    text = re.sub('\\w*\\d\\w*', '', tt)\n",
    "    text = re.sub('_+[^a-z]', '', text)\n",
    "    text = re.sub('_+[a-z]', '', text)\n",
    "    test = re.sub('@[a-zA-Z0-9]*', '', text)\n",
    "    text = re.sub('[''\"\"...]', '', text)\n",
    "    text = text.replace(\",\", \"\")\n",
    "    text = re.sub(\"[''\"\"’’...**][a-zA-Z0-9]*\", '', text)\n",
    "    text = re.sub('@[a-zA-Z0-9]*', '', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Uses the common stop words provided by `nltk`\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    # Sets up regex to filter out stop words    \n",
    "    pattern = re.compile(r'\\b(' + r'|'.join(stop) + r')\\b\\s*')\n",
    "\n",
    "    return pattern.sub('', text)\n",
    "\n",
    "tweets['cleaned_text'] = tweets['text'].apply(lambda x: remove_stopwords(clean_data(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binarization\n",
    "\n",
    "I also one-hot encoded the emotions from the tweets. This was done because I wanted to try creating a NN for the project. However, to do this I split the dataset into `train_tweets` and `test_tweets`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing, metrics, decomposition, pipeline, dummy\n",
    "\n",
    "# Split dataset\n",
    "test_tweets = tweets[tweets['identification'] == 'test']\n",
    "train_tweets = tweets[tweets['identification'] == 'train']\n",
    "\n",
    "# Create labels\n",
    "label_binarizer = preprocessing.LabelBinarizer()\n",
    "label_binarizer.fit(train_tweets.emotion)\n",
    "\n",
    "# One hot encode emotions\n",
    "train_tweets['bin_emotion'] = label_binarizer.transform(train_tweets['emotion']).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>crawldate</th>\n",
       "      <th>hastags</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>identification</th>\n",
       "      <th>emotion</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>bin_emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1205566.0</td>\n",
       "      <td>391</td>\n",
       "      <td>2015-05-23 11:42:47</td>\n",
       "      <td>[Snapchat]</td>\n",
       "      <td>0x376b20</td>\n",
       "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
       "      <td>train</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>People post add #Snapchat must dehydrated Cuz ...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1083048.0</td>\n",
       "      <td>433</td>\n",
       "      <td>2016-01-28 04:52:09</td>\n",
       "      <td>[freepress, TrumpLegacy, CNN]</td>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>@brianklaas As we see, Trump is dangerous to #...</td>\n",
       "      <td>train</td>\n",
       "      <td>sadness</td>\n",
       "      <td>As see Trump dangerous #freepress around worl...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1178309.0</td>\n",
       "      <td>376</td>\n",
       "      <td>2016-01-24 23:53:05</td>\n",
       "      <td>[]</td>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>Now ISSA is stalking Tasha 😂😂😂 &lt;LH&gt;</td>\n",
       "      <td>train</td>\n",
       "      <td>fear</td>\n",
       "      <td>Now ISSA stalking Tasha 😂😂😂 &lt;LH&gt;</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>687882.0</td>\n",
       "      <td>120</td>\n",
       "      <td>2015-06-11 04:44:05</td>\n",
       "      <td>[authentic, LaughOutLoud]</td>\n",
       "      <td>0x1d755c</td>\n",
       "      <td>@RISKshow @TheKevinAllison Thx for the BEST TI...</td>\n",
       "      <td>train</td>\n",
       "      <td>joy</td>\n",
       "      <td>Thx BEST TIME tonight What stories! Heartbre...</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>407631.0</td>\n",
       "      <td>1021</td>\n",
       "      <td>2015-08-18 02:30:07</td>\n",
       "      <td>[]</td>\n",
       "      <td>0x2c91a8</td>\n",
       "      <td>Still waiting on those supplies Liscus. &lt;LH&gt;</td>\n",
       "      <td>train</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>Still waiting supplies Liscus &lt;LH&gt;</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           score            crawldate                        hastags  \\\n",
       "1205566.0    391  2015-05-23 11:42:47                     [Snapchat]   \n",
       "1083048.0    433  2016-01-28 04:52:09  [freepress, TrumpLegacy, CNN]   \n",
       "1178309.0    376  2016-01-24 23:53:05                             []   \n",
       "687882.0     120  2015-06-11 04:44:05      [authentic, LaughOutLoud]   \n",
       "407631.0    1021  2015-08-18 02:30:07                             []   \n",
       "\n",
       "           tweet_id                                               text  \\\n",
       "1205566.0  0x376b20  People who post \"add me on #Snapchat\" must be ...   \n",
       "1083048.0  0x2d5350  @brianklaas As we see, Trump is dangerous to #...   \n",
       "1178309.0  0x1cd5b0                Now ISSA is stalking Tasha 😂😂😂 <LH>   \n",
       "687882.0   0x1d755c  @RISKshow @TheKevinAllison Thx for the BEST TI...   \n",
       "407631.0   0x2c91a8       Still waiting on those supplies Liscus. <LH>   \n",
       "\n",
       "          identification       emotion  \\\n",
       "1205566.0          train  anticipation   \n",
       "1083048.0          train       sadness   \n",
       "1178309.0          train          fear   \n",
       "687882.0           train           joy   \n",
       "407631.0           train  anticipation   \n",
       "\n",
       "                                                cleaned_text  \\\n",
       "1205566.0  People post add #Snapchat must dehydrated Cuz ...   \n",
       "1083048.0   As see Trump dangerous #freepress around worl...   \n",
       "1178309.0                   Now ISSA stalking Tasha 😂😂😂 <LH>   \n",
       "687882.0     Thx BEST TIME tonight What stories! Heartbre...   \n",
       "407631.0                  Still waiting supplies Liscus <LH>   \n",
       "\n",
       "                        bin_emotion  \n",
       "1205566.0  [0, 1, 0, 0, 0, 0, 0, 0]  \n",
       "1083048.0  [0, 0, 0, 0, 0, 1, 0, 0]  \n",
       "1178309.0  [0, 0, 0, 1, 0, 0, 0, 0]  \n",
       "687882.0   [0, 0, 0, 0, 1, 0, 0, 0]  \n",
       "407631.0   [0, 1, 0, 0, 0, 0, 0, 0]  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "\n",
    "After the data cleaning and preparation is finished, we split the training data into test and train data. This is done so we have testing data to validate our models against."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# We do this using the `train_tweets` dataframe. The `test_tweets` dataframe is the data that will be used for \n",
    "# the Kaggle competition predictions.\n",
    "sample_tweets = train_tweets.sample(n=1000000)\n",
    "\n",
    "tweets_x = sample_tweets['cleaned_text']\n",
    "tweets_y = sample_tweets['emotion']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(tweets_x, tweets_y, random_state=42, test_size=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the feature engineering step, we use both the bag of words approached and TF/IDF. These features will be used\n",
    "to train the various models since they need numerical data to train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bag of Words features via CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_vector = TfidfVectorizer(max_features=20000, min_df=1, stop_words='english', tokenizer=nltk.word_tokenize)\n",
    "\n",
    "# learn training data vocab and create a document-term matrix\n",
    "x_bow_traincv = bow_vector.fit_transform(x_train)\n",
    "\n",
    "# Transform the testing data into a document-term matrix\n",
    "x_bow_testcv = bow_vector.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TFID Features via TFIDVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vector = TfidfVectorizer(max_features=20000, min_df=1, stop_words='english', tokenizer=nltk.word_tokenize)\n",
    "\n",
    "# learn training data vocab and create a document-term matrix\n",
    "x_tfidf_traincv = tfidf_vector.fit_transform(x_train)\n",
    "\n",
    "# Transform the testing data into a document-term matrix\n",
    "x_tfidf_testcv =tfidf_vector.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform feature creation the tweets to be used in the Kaggle competition\n",
    "kaggle_x_tfidf_cv = tfidf_vector.transform(test_tweets['cleaned_text'])\n",
    "kaggle_x_bow_cv   = bow_vector.transform(test_tweets['cleaned_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models and in sights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Tree\n",
    "\n",
    "One of my first attempts was to train a random forest decision tree to peform the predictions. The Random Forest was trained with 75 n estimators and achieved an accuraccy of .43 on the original kaggle competitiion. It was trained with the TFIDF features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Naive Bayes Classifier\n",
    "\n",
    "I also trained two Naive Bayes classifier: one with the CountVectorizer features and the other with the TFIDF features. These two classifiers can be found below. Overall both models seem to have similar results so the different\n",
    "features did not make that much of a difference.\n",
    "\n",
    "Insights: For the Naive Bayes models, I initially was getting accuracy between 30% and 40%. I could not get them to go higher. I think my issue was that I was performing TOO much data cleaning on the tweets. So after stripping back on the data cleaning I managed to pass 40% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.57 s, sys: 63.7 ms, total: 2.63 s\n",
      "Wall time: 2.67 s\n",
      "Testing accuracy:  0.5095025\n",
      "CPU times: user 2.52 s, sys: 17.1 ms, total: 2.54 s\n",
      "Wall time: 2.55 s\n",
      "Testing accuraccy:  0.5095025\n"
     ]
    }
   ],
   "source": [
    "# TFIDF Model\n",
    "tfidf_mnb = MultinomialNB()\n",
    "\n",
    "# Train model\n",
    "%time tfidf_mnb.fit(x_tfidf_traincv, y_train)\n",
    "\n",
    "# Validate model accuracy\n",
    "tfidf_prediction = tfidf_mnb.predict(x_tfidf_testcv)\n",
    "\n",
    "print('Testing accuracy: ', metrics.accuracy_score(y_test, tfidf_prediction))\n",
    "\n",
    "# BOW Model\n",
    "bow_mnb = MultinomialNB()\n",
    "\n",
    "# Train model\n",
    "%time bow_mnb.fit(x_bow_traincv, y_train)\n",
    "\n",
    "# Validate model accuracy\n",
    "bow_prediction = bow_mnb.predict(x_bow_testcv)\n",
    "\n",
    "print('Testing accuraccy: ', metrics.accuracy_score(y_test, bow_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_submission(t_prediction, tweet_ids, name):\n",
    "    dataframe = pd.DataFrame({ 'id': tweet_ids.tolist(), 'emotion': t_prediction.tolist() })\n",
    "    dataframe.to_csv(name, index=None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_submission(tfidf_mnb.predict(kaggle_x_tfidf_cv), test_tweets['tweet_id'], 'naive_with_tfidf.csv')\n",
    "prepare_submission(bow_mnb.predict(kaggle_x_bow_cv), test_tweets['tweet_id'], 'naive_with_bow.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression\n",
    "\n",
    "I also trained a logistic regression model to see if the results can be improved. Overall, the logistic regression model's accuraccy was slightly better than the Naive Bayes models.\n",
    "\n",
    "Insights: Initially, the logistic regression model's results were almost the same as the naive bayes. However, after changing the `C` value and `max_iter` I managed to get the model to perform better than the naive bayes models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training 0.5769066666666667\n",
      "testing 0.51961\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logisticRegr = LogisticRegression(C=1e4, solver='lbfgs', multi_class='multinomial', max_iter=550)\n",
    "\n",
    "# Train model\n",
    "logisticRegr.fit(x_bow_traincv, y_train)\n",
    "\n",
    "# Validate model\n",
    "print('training', logisticRegr.score(x_bow_traincv, y_train))\n",
    "print('testing', logisticRegr.score(x_bow_testcv, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['logistic_regresssion_2.joblib']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg_prediction = logisticRegr.predict(kaggle_x_bow_cv)\n",
    "\n",
    "# Export\n",
    "prepare_submission(log_reg_prediction, test_tweets['tweet_id'], 'logreg_with_bow_2.csv')\n",
    "\n",
    "# Dump the model\n",
    "dump(logisticRegr, 'logistic_regresssion_2.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Network\n",
    "\n",
    "I also built a model using a Neural Network built with Keras. However, the results for the neural network wasn't as good as I expected it to be. The highest accuraccy I managed to get was 42%. \n",
    "\n",
    "I think this is a combination of things that I did not do too well. Firstly, it took way too long to train and I kept running out of time from my Google Colab session. I then tried to use Keras' Callback to save the model after each epoch of training. However, that still did not help with the training.\n",
    "\n",
    "I also tried to reduce amount of data cleaning I had performed on the results.\n",
    "\n",
    "But in the end, I think the logistic regression model is the model I went for and tried to improve the most."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Model Explanation\n",
    "\n",
    "In the end, my best results were obtained using Logistic Regression and the TFIDF features. To get the best results from the this model, I tried different `C` and `max_iter` values."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
